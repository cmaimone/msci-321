---
output:
  html_document:
    df_print: paged
    code_download: TRUE
    toc: true
    toc_depth: 1
project:
  execute-dir: project
editor_options:
  chunk_output_type: console
---

# Statistics and Probability

```{r}
library(tidyverse)
library(janitor)
```

## Descriptive vs Inferential Statistics

[Descriptive statistics]{style="color:blue"} are used to **summarize and describe** the features of a dataset. It focuses on presenting the data in a manageable and understandable form. The goal is to provide a clear and concise overview of the data.

**Key elements of descriptive statistics:**

-   **Measures of central tendency:** Mean, median, mode

-   **Measures of variability (spread):** Range, variance, standard deviation, interquartile range

-   **Frequency distributions:** Tables or graphs showing how data points are distributed (e.g., histograms, bar charts)

-   **Percentiles and quartiles:** Indicating the relative standing of values within a dataset

**Example:** If you're looking at the test scores of 100 students in a class, descriptive statistics would help you find the average score, the highest and lowest scores, and how scores are spread out.

[Inferential statistics]{style="color:blue"} are used to **make predictions or generalizations** about a population based on a sample of data. It involves drawing conclusions from a dataset that are meant to apply to a broader group or population.

**Key elements of inferential statistics:**

-   **Hypothesis testing:** Testing a claim or hypothesis about a population (e.g., t-tests, chi-square tests)

-   **Confidence intervals:** Estimating the range within which a population parameter is likely to fall (e.g., the population mean)

-   **Regression analysis:** Predicting relationships between variables (e.g., linear regression, logistic regression)

-   **Sampling methods:** Using a sample of data to infer characteristics of the entire population

**Example:** If you wanted to know the average test score for all students in the country, you might take a sample from a few schools and use inferential statistics to estimate the national average, including a margin of error or confidence level.

### Calculating descriptive statistics with R

For example, let's read in the airline safety dataset (source: https://github.com/fivethirtyeight/data/tree/master/airline-safety)

```{r}
air <- read.csv("data/airline_safety.csv")
names(air)
View(air)
```

*airline*: Airline (asterisk indicates that regional subsidiaries are included)

*avail_seat_km_per_week*: Available seat kilometers flown every week

*incidents_85_99*: Total number of incidents, 1985--1999

*fatal_accidents_85_99*: Total number of fatal accidents, 1985--1999

*fatalites_85_99*: Total number of fatalities, 1985--1999

*incidents_00_14*: Total number of incidents, 2000--2014

*fatal_accidents_00_14*: Total number of fatal accidents, 2000--2014

*fatalities_00_14*: Total number of fatalities, 2000--2014

### EXERCISE

Find the mean number of air fatalities in the 2000-2014 period

```{r}

```

Sometimes calculating the trimmed mean is useful to ignore extreme outliers:

```{r}
my_data <- c(2,3,4,5,6,7,8,9,10, -100)
mean(my_data)
mean(my_data, trim=0.10) # 10% trimmed mean
```

Let's look at some measures of variability:

The ***range*** of a variable is very simple: it's the biggest value minus the smallest value. The ***interquartile range*** (IQR) is like the range, but instead of calculating the difference between the biggest and smallest value, it calculates the difference between the 25th quantile and the 75th quantile. The 10th ***quantile (**or **percentile)*** of a data set is the smallest number x such that 10% of the data is less than x.

```{r}
range(air$incidents_00_14) # range
min(air$incidents_00_14)
max(air$incidents_00_14)
IQR(air$incidents_00_14) # inter-quartile range
quantile(air$incidents_00_14, probs=c(0.25, 0.75))
```

You could also decide to look at a frequency table instead:

```{r}
table(air$incidents_00_14)
hist(air$incidents_00_14, breaks = -1:25)
```

Similarly, some commonly used measures for variability are:

```{r}
sd(air$incidents_00_14) # standard deviation
var(air$incidents_00_14) # variance
mad(air$incidents_00_14) # median absolute deviation
```

You can also get an overall summary as:

```{r}
summary(air$incidents_00_14)
```

## Probability Distributions

A **probability distribution** is like a list or a map that tells you the probability (or chance) of something happening in a certain situation. In other words, it shows you all the possible outcomes of an event and how likely each one is.

**Example: Rolling a Die**

Let's say you're rolling a 6-sided die. The probability distribution for rolling this die would look something like this:

-   The probability of rolling a 1 is 1 out of 6 (because the die has 6 sides, and each side is equally likely to land face up).

-   The probability of rolling a 2 is also 1 out of 6.

-   The probability of rolling a 3 is 1 out of 6.

-   And the same for 4, 5, and 6.

So, the probability distribution for rolling a fair die looks like this:

-   Roll a 1: 1/6 chance

-   Roll a 2: 1/6 chance

-   Roll a 3: 1/6 chance

-   Roll a 4: 1/6 chance

-   Roll a 5: 1/6 chance

-   Roll a 6: 1/6 chance

Probability distributions can apply to other things too, not just dice. For example:

-   If you're flipping a coin, the probability distribution tells you that you have a 50% chance of getting heads and a 50% chance of getting tails.

-   If you're picking a card from a deck, the distribution would tell you the chances of drawing a certain card.

### **Why is it Important?**

The probability distribution helps you understand and predict the likelihood of different outcomes. The critical point is that probabilistic questions start with a known ***model*** of the world, and we use that model to do some calculations. It tells you which outcomes are more likely or less likely, and it's helpful in making decisions. So in simple terms, **a probability distribution is like a map that shows you the chances of different things happening.**

### **Some useful** probability **distributions**

As you might imagine, probability distributions vary enormously, and there's an enormous range of distributions out there. However, they aren't all equally important. In fact, the vast majority of statistical tests rely on one of five distributions: binomial, normal, t, χ2 ("chi-square") and F.

Let's look at some of these:

-   **Uniform**: when all outcomes are equally likely over a given interval. Eg: rolling a dice.
-   **Normal (Gaussian) Distribution**: The normal distribution is one of the most important in statistics. It's symmetric and described by its mean (`mu`) and standard deviation (`sigma`). It has a bell-shaped curve, with most values clustering around the mean. Often used for modeling errors, heights, test scores, etc.
-   **Student's t Distribution**: It is similar to the normal distribution but has heavier tails. The t-distribution is used when the sample size is small, and the population standard deviation is unknown. Used for hypothesis testing when the sample size is small and the population variance is unknown.
-   **Binomial Distribution**: The binomial distribution describes the number of successes in a fixed number of trials, each with the same probability of success. It is used for discrete events with two outcomes (success/failure). Eg: Coin flips, success/failure experiments, diseased/healthy.
-   **Poisson Distribution**: used for modeling the number of events occurring in a fixed interval of time or space. It is used when events happen independently and at a constant average rate. Eg: Modeling phone calls received at a call center, the number of accidents at a traffic intersection.
-   **Exponential Distribution**: used to model the time between events in a Poisson process. It has a memoryless property, meaning the probability of an event occurring in the future is independent of the past. Eg: Modeling the time until a phone call is received, or the time until cancer reoccurence.
-   **Chi-Squared Distribution**: used for hypothesis testing, particularly in tests of independence and goodness of fit. It helps us figure out if something unusual or surprising is happening in our data. It's mostly used when we want to see if things are different from what we expect, like if the outcome of an experiment is what we thought it would be.
-   **F-Distribution**: used to compare variances between two or more groups or models. The F-distribution is defined by two parameters: degrees of freedom for the numerator and denominator. These degrees of freedom are based on the number of groups or variables involved in the analysis. Used particularly in the analysis of variance (ANOVA) and regression analysis.

### EXERCISE

What would be the best probability distribution to model the number of incidents from 2000-2014 in our airline safety dataset? Why?

```{r}

```

## **Sampling and Random Samples**

[Objective]{.underline}: **Understand how data is collected** and **why randomness matters** in drawing conclusions about a population.

### **1. Population, Sample, and Random Sampling**

-   **Population**: The entire group that you're interested in studying. It can be large or small, depending on the context.

    -   **Example**: All the students in a university, all patients with a specific disease, or all the trees in a forest.

-   **Sample**: A subset of the population selected for the study. Ideally, it should represent the population well.

    -   **Example**: A sample of 100 students from the university.

-   **Random Sampling**: A technique where each member of the population has an equal chance of being selected for the sample. This helps eliminate bias and ensures that the sample is representative of the population.

    -   **Why It Matters**: Random sampling helps avoid **selection bias** and ensures that your results can be generalized to the larger population.

### **2. Sampling Methods**

There are several different ways to select a sample from the population. Here are some common methods:

-   **Simple Random Sampling (SRS)**:

    -   **Description**: Every member of the population has an equal chance of being chosen.

    -   **Example**: Drawing names from a hat.

```{r}
# Simulate a population of 1000 people
population <- 1:1000

# Simple random sampling of 100 people
sample_srs <- sample(population, size = 100, replace = FALSE)
```

-   **Stratified Sampling**:

    -   **Description**: The population is divided into subgroups (strata) based on certain characteristics (e.g., age, gender), and a random sample is taken from each subgroup.

    -   **Example**: A survey about health might sample equally from different age groups (e.g., 18-30, 31-50, etc.

```{r}
# Create strata (age groups) within a population
age_groups <- c("18-30", "31-50", "51-70", "70+")
population <- data.frame(
  id = 1:1000, 
  age_group = sample(age_groups, 1000, replace = TRUE)
)

# Stratified sampling: 25 samples per age group
stratified_sample <- population %>%
  group_by(age_group) %>%
  sample_n(25)
```

### **3. Sampling Bias and Its Impact**

Sampling bias occurs when certain individuals or groups are systematically excluded or overrepresented in the sample. This can lead to inaccurate or misleading results.

-   **Types of Bias**:

    -   **Selection Bias**: Certain members of the population are more likely to be chosen (e.g., only surveying people available during the day).

    -   **Non-response Bias**: When individuals selected for the sample do not respond (e.g., people who don't answer a survey may have different opinions from those who do).

    -   **Survivorship Bias**: Focusing on only those who survived or succeeded (e.g., studying successful businesses without considering the failed ones).

-   **How to Minimize Bias**:

    -   Use **random sampling** methods.

    -   Ensure that the sample **includes all relevant subgroups**.

    -   Try to **increase response rates** in surveys to reduce non-response bias.

### **4. Why Randomness Matters**

Random sampling is crucial for **generalizability** and **validity**:

-   **Generalizability**: A well-chosen random sample allows you to make inferences about the population, not just the sample.

-   **Validity**: Randomness ensures that there are no hidden patterns or biases in the way you collect your data, making your results more reliable.

### **Summary**

-   **Population**: The entire group you're interested in.

-   **Sample**: A subset of the population.

-   **Random Sampling**: Every individual in the population has an equal chance of being selected, which minimizes bias.

-   **Sampling Methods**:

    -   **Simple Random Sampling**: Every individual has an equal chance of selection.

    -   **Stratified Sampling**: The population is divided into strata, and random samples are taken from each.

-   **Sampling Bias**: It occurs when certain individuals are over or underrepresented, which can affect the validity of your conclusions.

In R, using functions like `sample()`, `group_by()`, and `sample_n()` helps you efficiently implement various sampling techniques.

## Working with probability distribution functions in R

R provides four functions to work with every probability distribution that it implements. Each one calculates a different quantity of interest. No matter what distribution you're talking about, there's a `d` function, a `p` function, a `q` function and a `r` function.

| What it does                       | Prefix | Binomial distribution | Normal distribution |
|--------------------|-----------------|-----------------|-----------------|
| get the probability (density) of   | d      | `dbinom()`            | `dnorm()`           |
| find the cumulative probability of | p      | `pbinom()`            | `pnorm()`           |
| generate random number from        | r      | `rbinom()`            | `rnorm()`           |
| value at given quantile/percentile | q      | `qbinom()`            | `qnorm()`           |

: **Let's see an example:** Suppose you are studying the test scores of a class where the scores follow a normal distribution with a **mean of 75** and a **standard deviation of 10**. You want to:

1.  Generate 100 random test scores.
2.  Compute the probability density of a specific score (say, a score of 85).
3.  Find the score that corresponds to the top 10% of the class (i.e., the 90th percentile).
4.  Compute the cumulative probability of scoring below a specific score (say, below 75).

How might you implement this in R?

```{r}
# what is the probability distribution we should use?
# what are the parameters of this prob. dist.?
# what values should we give these params?

# STEP 1
# Generate 100 random test scores
set.seed(42)  # For reproducibility
random_scores <- rnorm(100, mean = 75, sd = 10)

# Display the first few random scores
head(random_scores)

# plot the simulated distribution
hist(random_scores)
```

Now, let's calculate the probability density (i.e., how likely it is to get a specific score like 85) using the `dnorm()` function.

```{r}
# STEP 2
# Calculate the density for a score of 85
score_density <- dnorm(85, mean = 75, sd = 10)
score_density
```

This will give you the value of the **normal density function** at 85, which represents the likelihood of observing a score of exactly 85 in the population. The **density** is not a probability but rather a height of the curve at that point. Probability is calculated as area under the curve for a given interval, eg: summing up the probability density for scores between 70-90 to get the total probability of scoring in that interval. As expected, the total area under a PDF should sum to 1.

To find the test score corresponding to the top 10% of the class (the 90th percentile), we can use the `qnorm()` function.

```{r}
# STEP 3
# Find the 90th percentile score
percentile_90 <- qnorm(0.90, mean = 75, sd = 10)
percentile_90

# you can also put in a vector of multiple values to check
percentile_values_to_check <- c(0.50, 0.67, 0.95, 0.99)
qnorm(percentile_values_to_check, mean = 75, sd = 10)
```

This will give you the score below which 90% of the students' scores lie. In other words, it tells you the **test score that corresponds to the 90th percentile**.

Conversely, you can also find out the percentile corresponding to a given score/value. Say, you want to find out the cumulative probability that a randomly selected student scores **below 75**.

```{r}
probability_below75 <- pnorm(75, mean = 75, sd = 10)
probability_below75
```

As expected, the probability that a random student scores below 75 is 0.5 or 50%.

### Practical example - modeling side effects of medications

Medical professionals use the binomial distribution to model the probability that a certain number of patients will experience side effects as a result of taking new medications. For example, suppose it is known that 5% of adults who take a certain medication experience negative side effects.

We can use the binomial distribution to find the probability that more than a certain number of patients in a random sample of 100 will experience negative side effects.

```{r}
# draw a random sample from a binomial distribution
side_effect_model <- rbinom(n=30, # how many replicates / trials 
                            size=100, # sample size per trial
                            prob=0.05) # prob of success i.e. side effects

# For every trial (total 30), the number of patients out of 100 that experienced side-effects
side_effect_model

hist(side_effect_model)
```

Remember, you want to calculate the cumulative probability P(X \> 5 patients experience side effects from a sample of 100). This is the same as 1 - P(X ≤ 5). We know how to calculate this quantity.

```{r}
# P(X ≤ 5)
p_less_than_or_equal_to_5 <- pbinom(q=5,      # value (num patients with side effects)
                                    size=100, # sample size of total patients
                                    prob = 0.05) # prob of side effects
 
p_less_than_or_equal_to_5

# P(X > 5)
p_greater_than_5 = 1 - p_less_than_or_equal_to_5
p_greater_than_5
```

Similarly for P(X \>10) and P(X \> 15):

```{r}
1 - pbinom(q=10, size=100, prob = 0.05)
1 - pbinom(q=15, size=100, prob = 0.05)
```

This gives medical professionals an idea of how likely it is that more than a certain number of patients will experience negative side effects.

### EXERCISE

The distribution of ACT scores for high school students in the U.S. is normally distributed with a mean of 21 and a standard deviation of about 5. What is the score of a student at the 99th percentile?

```{r}

```

### EXERCISE

The distribution of diastolic blood pressure is normally distributed with a mean of about 80 and a standard deviation of 20. If your blood pressure is 72, what percentage of the population has a blood pressure *greater* than yours?

```{r}

```

### EXERCISE

A six-sigma observation is an observation that is 6 standard deviations away from the mean. What is the probability of observing a value greater than or equal to the six-sigma value for a normally distributed variable? Assume mean=0, sd=1.

```{r}

```

## Central Limit Theorem

Suppose you want to make inferences about the mean of a population. For example, say you want to estimate the average height of a human. The population would be a representative sample of humans. From this sample, you could estimate a mean value - but how near (or far) is this sample mean from the true mean of the human population? There will be some random sampling noise in this mean - how could you make this estimate better?

Enter Central Limit Theorem:

-   **Regardless of the population distribution** (whether normal or not), if we take sufficiently large random samples (typically n ≥ 30), the **sampling distribution of the sample mean** will approach a **normal distribution**.

-   The mean of the sampling distribution will be equal to the mean of the population.

-   The standard deviation of the sampling distribution (called the **Standard Error**, SE) will be equal to the population standard deviation divided by the square root of the sample size $SE = \frac{\sigma}{\sqrt(n)}$

-   CLT is fundamental when drawing conclusions from clinical trials, public health data, genetic studies, or any research where data collection is based on samples rather than whole populations.

    ### **Key Assumptions for CLT**:

    1.  **Random Sampling**: The data should be collected randomly to avoid bias.

    2.  **Sample Size**: For populations that are not normally distributed, a larger sample size is generally required (n ≥ 30 is a common rule of thumb).

    3.  **Independent Samples**: The samples should be independent of one another.

Let's run a simulation in R to visualize the Central Limit Theorem.

```{r}
set.seed(42)

# Parameters
population_size <- 100000  # Population size
sample_size <- 30  # Sample size
num_samples <- 1000  # Number of samples

# Create a non-normal population (uniform distribution)
population <- runif(population_size, min = 0, max = 100)
population
population %>%
  hist(main = "Histogram of Population", xlab = "Value", col = "lightblue", border = "white")
mean(population) # this is the true mean!


# Draw 1 sample from the population to estimate the pop. mean
sample <- sample(population, sample_size, replace = TRUE)
sample
sample_mean <- mean(sample)
sample_mean # this is sample mean from one sample


# Draw a 1000 samples from the population, and calculate their sample means
sample_means <- 1:num_samples %>%
  purrr::map_dbl(~mean(sample(population, sample_size, replace = TRUE)))
sample_means
sample_means %>%
  hist(main = "Sampling Distribution of the Mean", xlab = "Sample Mean", col = "lightgreen", border = "white")

# Plots side-by-side for comparision
par(mfrow = c(1, 2))  # Split plotting area into 2
hist(population, 
     main = "Population Distribution (Uniform)", xlab = "Value", col = "lightblue", border = "white")
hist(sample_means, 
     main = "Sampling Distribution of the Mean", xlab = "Sample Mean", col = "lightgreen", border = "white")
par(mfrow=c(1,1)) # reset par 

# Show the mean of the sampling distribution
mean(sample_means)

# Let's compare to the true population mean
mean(population)
```

The mean of sample means approximates the true population mean. But what is the range of error in our estimate of the true mean?

### Confidence Intervals

CLT dictates that the sampling distribution of the sample mean is approximately normal, regardless of the population's distribution (if the sample size is large enough). This implies that we can use the known properties of the normal distribution to provide error ranges to our estimate of the population mean.

A **confidence interval (CI)** is a range of values that is likely to contain the population parameter (like the population mean) with a certain level of confidence, typically 95% or 99%.

For example:

-   A **95% confidence interval** means we are 95% confident that the true population parameter (e.g., the true mean) lies within this range.
-   Because of the **Central Limit Theorem**, the sampling distribution of the sample mean is approximately normal, regardless of the population's distribution (if the sample size is large enough).
-   This allows us to calculate confidence intervals for the population mean, [because we can assume that the sample means follow a normal distribution.]{.underline}

![](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/1_empirical-rule.jpg){width="442"}

**The Formula for Confidence Interval:**

For a 95% confidence interval, we typically use the following formula:

$$
\bar{x}±z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt(n)}
$$Where:

-   $\bar{x}$ = sample mean

-   $z_{\frac{\alpha}{2}}$ = Z value for the confidence level (for 95%, it's 1.96)

-   $\sigma$ = population standard deviation (if known) or sample standard deviation (if not)

-   $n$ = sample size

### Detour: Z-scores

A Z score is a measure that describes how many standard deviations a data point (or sample mean) is away from the mean of a population or distribution.

$$
z = \frac{x - \mu}{\sigma}
$$

**Why is the Z-Value Useful?**

-   **Comparing values across different distributions:** The z-value allows you to compare scores from different distributions, even if the distributions have different means and standard deviations.

-   **Standard normal distribution:** When the z-value is calculated for any data point in a normal distribution, it is standardized so that the distribution of z-values will have a mean of 0 and a standard deviation of 1 (known as the **standard normal distribution**).

-   In the context of confidence intervals, the z-value represents how many standard deviations away from the mean you need to go to capture a certain percentage of the population's data. For example:

    -   For a **95% confidence interval**, the corresponding **z-value** is 1.96, which means you would expect the true population parameter to lie within 1.96 standard deviations of the sample mean.

Let's see how z-scores, central limit theorem and confidence intervals tie in together:

```{r}
# Parameters
set.seed(123)  # For reproducibility

# Create a uniform population
population_size <- 100000
population <- runif(population_size, min = 0, max = 100)  # Uniform distribution
hist(population)

# Take a sample from the population
sample_size <- 30
sample_data <- sample(population, sample_size)
hist(sample_data)

# Calculate the sample mean and standard deviation
sample_mean <- mean(sample_data)
sample_mean
sample_sd <- sd(sample_data)
sample_sd

# Calculate the 95% confidence interval around the sample mean using the formula for CI
z_value <- 1.96  # For 95% confidence level
error_margin <- z_value * (sample_sd / sqrt(sample_size))
error_margin

# Confidence Interval
ci_lower <- sample_mean - error_margin
ci_upper <- sample_mean + error_margin

# Output the results
cat("Sample Mean: ", sample_mean, "\n")
cat("Confidence Interval: [", ci_lower, ", ", ci_upper, "]\n")
```

-   **Real-World Use:** Confidence intervals are used in biostatistics to estimate parameters (like means, proportions) in real-world data. For example, a researcher may want to estimate the average blood pressure in a population, and the confidence interval helps to understand the range in which the true mean likely falls.

-   **Error Margin and Precision:** The width of the confidence interval reflects the precision of the estimate. A narrower CI means more precision, while a wider CI means less precision.

### EXERCISE

Convert the list of blood pressure readings from patients into z-scores

```{r}
blood_pressure <- c(115, 130, 125, 140, 150, 135, 145, 155, 160, 125)


```

### EXERCISE

You collected a random sample of 40 people's blood pressure measurements. The sample mean is 130 mmHg, and the sample standard deviation is 20 mmHg. Calculate a 95% confidence interval for the population mean blood pressure.

```{r}

```

### EXERCISE

Calculate the 95% confidence interval as before, assuming the sample size was 4000 instead of 40. What do you notice?

```{r}

```

## Hypothesis testing and p-values

In statistics, hypothesis testing is a way of making inferences or educated guesses about a population based on sample data. It allows us to test if the evidence supports a specific claim or theory.

Hypothesis tests are commonly used to determine whether a certain treatment has an effect, whether a difference exists between two groups, or whether a relationship exists between variables.

### **Key Concepts:**

1.  **Null Hypothesis (H₀):**

    -   The **null hypothesis** is the statement that there is no effect, no difference, or no relationship. It assumes that any observed difference or effect is due to random chance.

    -   Example: "There is no difference in the mean blood pressure between two groups."

2.  **Alternative Hypothesis (H₁ or Ha):**

    -   The **alternative hypothesis** is the statement that there is an effect, a difference, or a relationship.

    -   Example: "There is a difference in the mean blood pressure between two groups."

3.  **Test Statistic:**

    -   A test statistic is a number that is calculated from the sample data and used to decide whether to reject the null hypothesis. It helps determine **how far the sample statistic is from the population parameter**.

    -   Examples: t-statistic, z-statistic, chi-square statistic.

    -   Choosing the correct test statistic depends on several key factors, including the type of data (categorical, quantitative), the sample size (n \< 30, or n \> 30) the hypothesis you are testing (one-sample, association, etc), and the assumptions about the population and sample (eg: is the population standard deviation known or unknown).

4.  **P-value:**

    -   The **p-value** is the probability of observing the test statistic assuming the null hypothesis is true.

    -   A **low p-value** (typically \< 0.05) suggests strong evidence against the null hypothesis, leading us to reject H₀.

    -   A **high p-value** suggests weak evidence against H₀, so we fail to reject it.

5.  **Significance Level (α):**

    -   The significance level (α) is the threshold for deciding whether the p-value is sufficiently small to reject the null hypothesis. Common values for α are 0.05, 0.01, and 0.10.

    -   If **p \< α**, reject H₀. Otherwise, fail to reject H₀.

### **Steps in Hypothesis Testing:**

1.  **State the hypotheses** (H₀ and H₁).

2.  **Choose the significance level (α)** (commonly 0.05).

3.  **Select the appropriate test** (e.g., t-test, z-test).

4.  **Collect data** and compute the test statistic.

5.  **Find the p-value** associated with the test statistic.

6.  **Compare the p-value to α**:

    -   If p-value ≤ α, reject H₀.

    -   If p-value \> α, fail to reject H₀.

7.  **Make a conclusion** based on the comparison.

### **Types of Hypothesis Tests:**

-   **One-sample t-test:** Tests if the mean of a sample is different from a known value.

-   **Two-sample t-test:** Tests if the means of two independent samples are different.

-   **Paired t-test:** Tests if the means of two related samples are different.

-   **Chi-square test:** Tests for relationships between categorical variables.

-   **ANOVA (Analysis of Variance):** Tests if means differ between three or more groups.

### DEMO: different from known population characteristics

Say you want to know if your fasting blood glucose falls within the expected range or not. From the medical literature you find out that the population mean is 85 mg/dL with a standard deviation of 15 mg/dL. You measured your blood glucose as 125 mg/dL - how worried should you be?

**Null hypothesis (H₀):** Your blood glucose is equal to the population mean (85 mg/dL).

**Alternative hypothesis (H₁):** Your blood glucose is different from the population mean (85 mg/dL).

**test-statistic** The z-value (test statistic) tells us how many standard deviations your sample (your blood glucose level) is from the population mean.

**p-value** In R, we can use the `pnorm()` function to find the p-value associated with a z-score.

**alpha** Say our significance threshold is 0.05, if the p-value is below 0.05 we will worry!

```{r}
# Population parameters (assume normal distribution)
pop_mean = 85
pop_sd = 15

# your blood glucose
x = 125

# z-score
z = (x - pop_mean)/pop_sd

# P(X <= Z-score)
prob_below <-  pnorm(z, mean = 0, sd = 1)

# P (X > Z-score) = 1 - P(X <= Z-score)
1 - prob_below
```

Let's get a visual to understand this better

```{r}
# Set a seed for reproducibility
set.seed(42)

# Draw 1000 samples from a normal distribution with mean = 85 and sd = 15 (under the null hypothesis)
null_distribution <- rnorm(1000, mean = pop_mean, sd = pop_sd)
hist(null_distribution)

# Observed value (could be the mean of several observations as well)
observed_statistic <- 125

# Visualize the null distribution and observed statistic
library(ggplot2)
ggplot(data.frame(x = null_distribution), aes(x)) +
  geom_density(fill = "lightblue", alpha = 0.5) +
  geom_vline(xintercept = observed_statistic, color = "red", linetype = "dashed", size = 1) +
  labs(title = "Null Distribution and Observed Statistic",
       subtitle = paste("Observed Statistic = ", observed_statistic),
       x = "Blood glucose (mg/dL)", y = "Density") +
  theme_minimal()


# we could plot the same thing with z-scores
null_distribution <- rnorm(1000, mean = 0, sd = 1)
observed_statistic <- round(z, 4)
ggplot(data.frame(x = null_distribution), aes(x)) +
  geom_density(fill = "lightblue", alpha = 0.5) +
  geom_vline(xintercept = observed_statistic, color = "red", linetype = "dashed", size = 1) +
  labs(title = "Null Distribution and Observed Statistic",
       subtitle = paste("Observed Statistic = ", observed_statistic),
       x = "z-score", y = "Density") +
  theme_minimal()
```

### EXERCISE

You are conducting a weight loss study in which participants undergo a diet program. The average weight loss after 6 weeks for participants is 5 kg, with a standard deviation of 2 kg. One of the participants in your study lost 3 kg in 6 weeks.

You want to know if the participant's weight loss is typical, or if it's significantly low compared to the rest of the participants.

```{r}

```

### DEMO: Correlation test

Most of the time, R will implement functions that do a lot of the heavy-lifting for you, and give you an easy way to run hypothesis tests with a simple function. **`cor.test()`** is one such function to measure association or correlation between 2 variables.

Correlation measures the strength and direction of a linear relationship between two variables. The **correlation coefficient (r)** quantifies this relationship, 1 is perfect positive correlation, 0 is no correlation and -1 is perfect negative correlation.

**Types of Correlation Tests in `cor.test()`**:

-   **Pearson's Correlation**: Measures the linear relationship between two continuous variables.
-   **Spearman's Rank Correlation**: Used when the relationship between variables is not linear but can be ranked.
-   **Kendall's Tau**: A non-parametric test that measures the strength of association between two variables, also based on ranks.

The `cor.test()` function tests if the correlation coefficient is significantly different from zero.

`{cor.test(x, y, method = c("pearson", "kendall", "spearman"), alternative = c("two.sided", "less", "greater"), conf.level = 0.95)}`

**Example Problem:**

You want to know if there is a significant correlation between `incidents_85_99` and `incidents_00_14`

```{r}
# let's plot it
plot(air$incidents_85_99, air$incidents_00_14,
     pch = 16, col = "blue")

# run correlation test
cor_test <- cor.test(height, weight, method = "pearson")
```

Perform a correlation test as follows:

```{r}
# Step 1: Visualize the data with a scatter plot
plot(height, weight, main = "Height vs Weight",
     xlab = "Height (cm)", ylab = "Weight (kg)", 
     pch = 16, col = "blue")

# Step 2: Run the correlation test (Pearson's method)
cor_test <- cor.test(air$incidents_85_99, air$incidents_00_14, method = "pearson")
cor_test

```

How do you interpret the results? What happens if you change the method parameter?
