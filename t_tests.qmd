---
title: t-tests
output:
  html_document:
    df_print: paged
    code_download: TRUE
    toc: true
    toc_depth: 1
editor_options:
  chunk_output_type: console
---

```{r}
library(tidyverse)
library(palmerpenguins)
```


# Formulas in R

For many statistical tests in R, we can define the test in terms of a formula:

```         
outcome_variable ~ predictor_variable
```

The *precise* meaning of this formula depends on exactly what you want to do with it, but in broad terms it means "the outcome variable, analysed in terms of the predictor variable". That said, although the simplest and most common form of a formula uses the "one variable on the left, one variable on the right" format, there are others. For instance, the following examples are all reasonably common.

```         
out ~ pred1 + pred2                 # more than one variable on the right
out ~ pred1 + pred2 + pred1:pred2   # add an interaction term
out ~ pred1 * pred2                 # shorthand for the above (each term + interaction) 
~ var1 + var2                       # a 'one-sided' formula
```

Formulas also sometimes get used for other purposes than `outcome_variable ~ predictor_variable`

# t-tests

t-tests are one of the most commonly used statistical tests.  The "t" is from the t distribution, which is similar to the normal, but it has fatter tails (ends).  The t-distribution takes a parameter, degrees of freedom, which affects how fat the tails are.  As t increases, the t distribution becomes the normal distribution.  t-tests, which were created to study Guinness beer quality, account for the added uncertainty that comes when we have small samples (in terms of the number of observations).

There are three main variations:

* One sample: comparing a measure (often the mean) of a sample to a fixed value (often 0)
  * Two-sided null hypothesis: parameter = value; alternative: parameter != value 
* Two sample: comparing the value of a measure of one sample to that of another sample; the samples need to be independent.  Theoretically they should have the same variance as well, but in practice this often gets ignored
  * Two-sided null hypothesis: difference between sample 1 measure and sample 2 measure = 0; alternative: difference != 0
* Paired: for a set of subjects (patients, objects, etc), we have 2 measures for each subject.  Are the two measurements the same across the entire group?
  * Two-sided null hypothesis: difference between (the mean of) measurement 1 and (the mean of) measurement 2 = 0; alternative: difference != 0

We can also do one-sided tests: 

null hypothesis: parameter <= value; alternative: parameter > value 

if we have a reason to believe, AHEAD of the test or computation of values, that the difference can only be in one direction

We're going to focus on testing means.  But t-tests also get used to determine if estimated parameters from some statistical models are different from 0 or not.  

Note: we're talking in terms of frequentist statistics; Bayesian statistics use many of the same tests, but the framing/interpretation is different.


## One-sample t-test

Say that you know that the average weight of Adelie penguins across many studies is 3700g.  You have a sample of 30 Adelie penguins from a particular region.  Based on the information in your sample, does your population of penguins have a different average body weight than other Adelie penguins?


```{r}
set.seed(123)
adelie <- filter(penguins, species == "Adelie") %>%
  sample_n(30)

mean(adelie$body_mass_g)

t.test(adelie$body_mass_g, mu=3700) # one sample t-test
```

What we're asking is: if the mean mass of our regional Adelie penguins was actually in truth 3700g, how likely is it that we would observe the mean that we do in our sample (which will naturally vary some from the population mean because of normal variation and small sample sizes)?  

Our p-value tells us the proportion of the time that we'd get a value at least as extreme (because it's a two-sided test) as what we observed here.  We're looking at the absolute value of the difference between our sample mean and the value of interest.  

```{r}
adelie_centered <- mutate(adelie, body_mass_g = body_mass_g - 3700)
mean(adelie_centered$body_mass_g)

t.test(adelie_centered$body_mass_g, mu=0)
```

Similarly, if we were the same amount ABOVE 3700 instead of below, we get the same result:

```{r}
new_adelie <- mutate(adelie, body_mass_g = body_mass_g + 2*(3700 - mean(body_mass_g)))
mean(new_adelie$body_mass_g)
t.test(new_adelie$body_mass_g, mu = 3700)
```

Our test result gives us the t statistic.  It's computing the p-value for us, but we can also look this up using the function for the t-distribution.

The t.test() function returns an object that we can save.  It has multiple components (kind of like a data frame has multiple columns):

```{r}
t.test(adelie$body_mass_g, mu=3700) %>% names()

t.test(adelie$body_mass_g, mu=3700)$statistic

result <- t.test(adelie$body_mass_g, mu=3700)

pt(result$statistic, df=result$parameter) * 2  # because it's a two-sided test

result$p.value
```

The p-value is how much density is in the two tails:

```{r}
tvals <- seq(-3, 3, .1)
plot(dt(tvals, df=29) ~ tvals, type="l")
abline(v=result$statistic, col="red")
abline(v=-1* result$statistic, col="red")
```

Typically, we use 0.05 as a cut-off for statistical significance, but this is NOT a magical value.  0.051 vs. 0.049 is NOT practically meaningful.

Our p-value of 0.17 means that if the true mean mass of our population of penguins was 3700, then we would expect to see a sample mean AT LEAST AS EXTREME as what we observed (3595, or 105 away from 3700) 17% of the time (if we repeated a bunch of samples).


### EXERCISE

The code below takes a sample of Chinstrap penguins.  Is the mean bill_depth_mm of the sample statistically different than 18.5?

```{r}
chinstrap <- filter(penguins, species == "Chinstrap")
set.seed(343)
chinstrap_sample <- sample_n(chinstrap, 20)


```



## Two-sample t-test

Is the average weight of male and female penguins the same?  

```{r}
gentoo <- filter(penguins, species == "Gentoo")

count(gentoo, sex)

t.test(gentoo$body_mass_g[gentoo$sex == "male"],
       gentoo$body_mass_g[gentoo$sex == "female"])

# or 
t.test(gentoo$body_mass_g ~ gentoo$sex)
```

It drops rows/observations with missing values.

This result says "Welch Tow Sample t-test".  This is adjusting for the possibility that the variance of the two samples is not equal.  This is why there's a fractional degrees of freedom reported: because the correction affects this.  


### EXERCISE

Does the bill length of chinstrap penguins differ by sex?

```{r}
chinstrap <- filter(penguins, species == "Chinstrap")



```


## Paired t-test

Finally, we come to the paired samples t-test. 

Say we want to know whether people slept more when taking a drug than when not.  There is variation across people, so we're interested in the average change by person.  

```{r}
sleep_hours <- data.frame(id = 1:12, 
                          before = c(8, 7.9, 6.8, 9.2, 6, 7.5, 8.7, 7.6, 6.8, 7.3, 7.4, 8.1),
                          after = c(8.1, 7.8, 7, 9, 6.5, 7.8, 8.9, 7.5, 6.9, 7.6, 7.1, 8.2)
                          )
```


```{r}
t.test(sleep_hours$before, sleep_hours$after, paired=TRUE)
```

This is the same as computing the difference and testing that as a one-sample test:

```{r}
sleep_hours$difference <- sleep_hours$after - sleep_hours$before

t.test(sleep_hours$difference)  # mu = 0 is the default
```



# Testing proportions

t-tests are appropriate when we are working with continuous variables.  If we're working with proportions, the values are constrained between 0 and 1, so we need to make an adjustment to the test.  If we want to do a t-test, but the value that we're comparing is a proportion, we use prop.test().

Let's flip a coin:

```{r}
set.seed(3842)
flips <- sample(c("heads", "tails"), replace=TRUE, 10)
flips
table(flips)
```

If we saw this frequency of heads and tails (proportion of .8), do we think this is a fair coin?

```{r}
prop.test(x = 2,  # number of successes (we'll pick tails to use)
          n = 10  # number of trials = flips
          )

# or
prop.test(table(flips))
```

Note that we get a different test statistic here: chi-squared (in output looks like an X, in greek letters it's sometimes looks like a script x).

That is because we're now working with counts (proportions are counts).  More on that in the next file.  



# t-tests in Tidyverse

Can we do t-tests as part of a tidyverse workflow?  Yep.

```{r}
library(infer)  # may need to install this
```


```{r}
# before:
t.test(gentoo$body_mass_g ~ gentoo$sex)

# with infer package
penguins %>%
  filter(species == "Gentoo") %>%
  t_test(body_mass_g ~ sex)

# group by does NOT work unfortunately
penguins %>%
  group_by(species) %>%
  t_test(body_mass_g ~ sex)
```







