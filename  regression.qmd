---
title: Regression Models
output:
  html_document:
    df_print: paged
    code_download: TRUE
    toc: true
    toc_depth: 1
editor_options:
  chunk_output_type: console
---

```{r}
library(tidyverse)
library(palmerpenguins)
```

# Regression

There are many types of regression models.  What ties them together is that they all model the relationship between an outcome variable (or dependent variable) and one or more predictor/explanatory/independent variables.  This can be for prediction or, more commonly in statistical analysis of existing data, to determine which predictor variables have a significant relationship with the outcome variable, controlling for the other factors.

The core regression model is called ordinary least squares (OLS) regression, and it is appropriate for a continuous outcome variable and any types of explanatory variables (discrete or continuous, or categorical -- which are operationalized as a series of binary indicator variables).  It's called ordinary least squares because we're fitting a straight line through points (in multidimensional space) such that we minimize the square of the distance between each point and the line (this distance is the error).

## Linear (OLS) regression

### Two-variables

We use the formula syntax, with the outcome variable on the left of the ~ and the predictor variable(s) on the right:

```{r}
lm(bill_depth_mm ~ bill_length_mm, data=penguins)
```

Specifying `data=` at the end allows us to not have to preface the variable names with the data frame name. 

(Remember, we should be controlling for species here as well -- we'll get to that.)

This is telling us that the best fit line between these two variables can be described by the equation:

```
bill_depth_mm = 20.89 - 0.085*bill_length_mm
```

The intercept is where the line hots the y-axis -- when bill_length_mm is 0, what's the value of bill_depth_mm.

```{r}
# we'll learn more ggplot in week 9
ggplot(penguins, aes(y=bill_depth_mm, x=bill_length_mm)) + 
  geom_point() + 
  geom_smooth(method="lm", se=FALSE)
```

We can get more information about the model with the summary function (same one we use with data frames -- it does different things depending on the input)

```{r}
summary(lm(bill_depth_mm ~ bill_length_mm, data=penguins))
```

It automatically drops observations that have a missing value on ANY of the variables used in the model.  

The output of summary tells us the standard error on the estimated coefficients, as well as conducting a t-test for each as to whether they are different from 0.  That's the p-value reported: the result of the t-test.  

R-squared is a measure of the goodness of fit and tells you how much of the variance in the outcome variable is explained by the predictor variable(s).  It runs between 0 and 1.  With just two variables in the model, this is the correlation between the variables, squared:

```{r}
cor(penguins$bill_depth_mm, penguins$bill_length_mm, use="pairwise")^2
```

Often, you can just read the result values off of the screen, but like with a t-test, you can get the values out of the objects that are returned:

```{r}
reg1 <- lm(bill_depth_mm ~ bill_length_mm, data=penguins)

names(reg1)

reg1$coefficients

sreg1 <- summary(reg1)

names(sreg1)

sreg1$r.squared
```

### EXERCISE

Run a linear regression predicting `bp.1s` by `bp.1d`

```{r}
diabetes <- read_csv("data/diabetes.csv")


```




### Order Matters

Note that regression models are not symmetrical.  If we solved the equation above for bill length as the outcome variable instead, we'd get:

```
bill_depth_mm = 20.89 - 0.085*bill_length_mm
bill_depth_mm - 20.89 = - 0.085*bill_length_mm
bill_length_mm = -20.89/-0.085 + (1/-0.085) * bill_depth_mm
bill_length_mm = 245.76 - 11.76 * bill_depth_mm
```

```{r}
ggplot(penguins, aes(y=bill_depth_mm, x=bill_length_mm)) +
  geom_point() + 
  geom_smooth(method="lm", se=FALSE, fullrange=TRUE) +
  xlim(NA, 300) +
  ylim(0, NA)
```


But that is not what we get:

```{r}
summary(lm(bill_length_mm ~ bill_depth_mm, data=penguins))
```


```{r}
ggplot(penguins, aes(y=bill_length_mm, x=bill_depth_mm)) +
  geom_point() + 
  geom_smooth(method="lm", se=FALSE, fullrange = TRUE) + 
  xlim(0, NA)
```



### Multiple predictors

Our model above isn't very good.  Let's add in more variables:

```{r}
reg2 <- lm(bill_depth_mm ~ bill_length_mm + flipper_length_mm + body_mass_g, 
           data = penguins)
summary(reg2)
```


### Categorical Variables

All 3 predictor variables above are continuous.  But we can add categorical variables as well.  

```{r}
reg3 <- lm(bill_depth_mm ~ bill_length_mm + flipper_length_mm + body_mass_g + sex, 
           data = penguins)
summary(reg3)
```

What happened here is that it automatically made an indicator variable where male = 1 and female = 0 (this is "sexmale" in the output).  So the results are giving us two parallel lines: one for male penguins and one for female penguins -- the slopes of the lines are the same, but the y-intercepts are different.  This is an intercept shift -- basically saying that male penguins start with a larger base value than female penguins.  The effect of being male is additive.

We can visualize this with a simpler model:

```{r}
reg4 <- lm(bill_depth_mm ~ bill_length_mm + sex, 
           data = penguins)
summary(reg4)

penguins %>%
  filter(!is.na(sex)) %>%
  ggplot(aes(y=bill_depth_mm, x=bill_length_mm, color = sex)) +
  geom_point() + 
  geom_abline(slope = -0.14576, intercept = 22.56139, color="#f8766D", linewidth=2) + # female
  geom_abline(slope = -0.14576, intercept = 22.56139 + 2.01334, color="#00BFC4", linewidth=2) # male
```

This is not the same result that we'd get if we ran separate models for each sex:

```{r}
penguins %>%
  filter(!is.na(sex)) %>%
  ggplot(aes(y=bill_depth_mm, x=bill_length_mm, color = sex)) +
  geom_point() + 
  geom_smooth(method="lm", se=FALSE)
```

When each group is computed separately, the slope are different.  We all this by using an interaction term in our regression model:

```{r}
reg5 <- lm(bill_depth_mm ~ bill_length_mm + sex + bill_length_mm:sex, 
           data = penguins)
summary(reg5)
```

Overall, this is:

```
bill_depth_mm = 22.999 + 1.18574*(male indicator) + -0.15614 * bill_length_mm + 0.01890 * bill_length_mm * (male indicator)
```

The male indicator variable is 1 for male, 0 for female.  So for female penguins, those terms drop.

The line for female penguins is:

```
bill_depth_mm = 22.999 - 0.15614 * bill_length_mm
```

The line for male penguins is:

```
bill_depth_mm = (22.999 + 1.18574) + (-0.15614 + 0.01890) * bill_length_mm
```

The slope of the line for male penguins is slightly less deep.

The shorthand for including 2 variables and their interaction is:

```{r}
reg5 <- lm(bill_depth_mm ~ bill_length_mm * sex, 
           data = penguins)
summary(reg5)
```


### EXERCISE

Write a linear regression model to predict `chol` that includes frame size as a predictor variable (along with others).  What do you make of the results?

```{r}

```


### Catgory Order

To change which category is the base category, we control the factor levels for the variable (here's where factors show up!)

```{r}
penguins$sexf <- factor(penguins$sex, levels = c("male", "female"))
reg6 <- lm(bill_depth_mm ~ bill_length_mm * sexf, 
           data = penguins)
summary(reg6)
```

Note that the base intercept is now higher, because it's for male penguins, and the intercept shift for female penguins is negative.  

### More than 2 Categories

```{r}
reg7 <- lm(bill_depth_mm ~ bill_length_mm + species, 
           data = penguins)
summary(reg7)
```

First, we've held the slope the same for all, but allowed intercept shifts.  But if we think the slope should vary too:

```{r}
reg8 <- lm(bill_depth_mm ~ bill_length_mm * species, 
           data = penguins)
summary(reg8)
```

We'd need to do some math, but the coefficient estimates are the same as running separate regressions.  

```{r}
summary(lm(bill_depth_mm ~ bill_length_mm, data = filter(penguins, species == "Adelie"))) # baseline
summary(lm(bill_depth_mm ~ bill_length_mm, data = filter(penguins, species == "Chinstrap")))
summary(lm(bill_depth_mm ~ bill_length_mm, data = filter(penguins, species == "Gentoo")))
```

### Additional Notes

You can also have interactions between continuous variables, but they get more difficult to interpret.  It's estimating a coefficient on multiplying two variables together.  

You can also include transformed variables (squared, logged, etc.) in a model.

When you have interaction terms or categorical variables in the model, the p-values are directly for the coefficient that is listed.  Interpreting whether a variable has an overall significant effect gets more complicated because you need to combine multiple coefficients together.  Computing these effects and their significance is beyond the scope of this course.  There are packages that can help with these tasks.  


## Logistic Regression

If your outcome variable is binary instead of continuous, then we use a logistic regression model.  It's one type of generalized linear model (GLM), so we use the glm() function. 

```{r}
penguins$heavy <- penguins$body_mass_g > 4000
l1 <- glm(heavy ~ sex + species + flipper_length_mm, data = penguins, family = "binomial")  # the binomial part indicates logit
summary(l1)
```

The coefficients no longer are giving you values in a direct linear equation (it is a line, but for predicting the logit of the outcome, not the direct value).

But interpretation of the direction and significance remains the same. 





